{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "convinced-poison",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Изучение-и-подготовка-данных-для-BERT\" data-toc-modified-id=\"Изучение-и-подготовка-данных-для-BERT-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Изучение и подготовка данных для BERT</a></span></li><li><span><a href=\"#Построение-моделей\" data-toc-modified-id=\"Построение-моделей-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Построение моделей</a></span><ul class=\"toc-item\"><li><span><a href=\"#Модель-логистической-регрессии\" data-toc-modified-id=\"Модель-логистической-регрессии-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Модель логистической регрессии</a></span></li><li><span><a href=\"#Модель-LightGBM\" data-toc-modified-id=\"Модель-LightGBM-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Модель LightGBM</a></span></li><li><span><a href=\"#Модель-CatBoost\" data-toc-modified-id=\"Модель-CatBoost-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Модель CatBoost</a></span></li></ul></li><li><span><a href=\"#Вывод-по-BERT:\" data-toc-modified-id=\"Вывод-по-BERT:-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Вывод по BERT:</a></span></li><li><span><a href=\"#Подготовка-данных-для-классификации-текстов\" data-toc-modified-id=\"Подготовка-данных-для-классификации-текстов-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Подготовка данных для классификации текстов</a></span></li><li><span><a href=\"#Построение-моделей\" data-toc-modified-id=\"Построение-моделей-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Построение моделей</a></span><ul class=\"toc-item\"><li><span><a href=\"#Модель-логистической-регрессии\" data-toc-modified-id=\"Модель-логистической-регрессии-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Модель логистической регрессии</a></span></li><li><span><a href=\"#Модель-LigthGBM\" data-toc-modified-id=\"Модель-LigthGBM-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Модель LigthGBM</a></span></li><li><span><a href=\"#Модель-CatBoost\" data-toc-modified-id=\"Модель-CatBoost-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Модель CatBoost</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Выводы</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-wyoming",
   "metadata": {},
   "source": [
    "## Изучение и подготовка данных для BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "designed-agency",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers as ppb\n",
    "from tqdm import notebook\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "specific-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Код для чтения данных\n",
    "try:\n",
    "    df_toxic = pd.read_csv('df_toxic.csv')\n",
    "except:\n",
    "    df_toxic= pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv')\n",
    "    df_toxic.to_csv('df_toxic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aggressive-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Будем работать только с 1000 строками ради сокращения времени выполнения кода\n",
    "df_toxic = df_toxic.sample(n=1000, random_state=12345).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "current-middle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ahh shut the fuck up you douchebag sand nigger...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"\\n\\nREPLY: There is no such thing as Texas Co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reply\\nHey, you could at least mention Jasenov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thats fine, there is no deadline )   chi?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"\\n\\nDYK nomination of Mustarabim\\n Hello! You...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Date warriors \\n\\nHi, Hertz. Nice catch on Sec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>REDIRECT Talk:2013 Men's World Ice Hockey Cham...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>This article is one of the most well cited art...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Harry the point you are making is pointless. I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>\"Moreover, \"\"a translation of the concept x in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  toxic\n",
       "0    Ahh shut the fuck up you douchebag sand nigger...      1\n",
       "1    \"\\n\\nREPLY: There is no such thing as Texas Co...      0\n",
       "2    Reply\\nHey, you could at least mention Jasenov...      0\n",
       "3            Thats fine, there is no deadline )   chi?      0\n",
       "4    \"\\n\\nDYK nomination of Mustarabim\\n Hello! You...      0\n",
       "..                                                 ...    ...\n",
       "995  Date warriors \\n\\nHi, Hertz. Nice catch on Sec...      0\n",
       "996  REDIRECT Talk:2013 Men's World Ice Hockey Cham...      0\n",
       "997  This article is one of the most well cited art...      0\n",
       "998  Harry the point you are making is pointless. I...      0\n",
       "999  \"Moreover, \"\"a translation of the concept x in...      0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pleased-cooperation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    1000 non-null   object\n",
      " 1   toxic   1000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_toxic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "classical-plaza",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, toxic]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Найдем дубликаты\n",
    "df_toxic[df_toxic.duplicated() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "binary-macintosh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    901\n",
       "1     99\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Найдем распределение оценок\n",
    "df_toxic['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pressed-alexandria",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd851c27446b4535b88828654287546e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Загрузка предобученной модели DistilBERT\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel,\n",
    "                                                    ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "# Загрузка предобученной модели/токенизатора \n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "minor-reasoning",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (862 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Токенизируем каждый комментарий\n",
    "tokenized = df_toxic['text'].apply((lambda x:\n",
    "                                    tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "maritime-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Найдем максимальную длину векторов после токенизации\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "# Применим padding к векторам\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "educational-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим маску для выделения важных токенов\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "disabled-display",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4645)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "affiliated-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ограничим количество токенов\n",
    "padded_01 = padded[:,:512]\n",
    "attention_mask_01 = attention_mask[:,:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "exterior-tiger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049df3701df64292bf8ce21c37839035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Сделаем цикл по батчам\n",
    "batch_size = 100\n",
    "embeddings = []\n",
    "for i in notebook.tqdm(range(padded_01.shape[0] // batch_size)):\n",
    "        # Преобразуем данные в формат тензоров\n",
    "        batch = torch.LongTensor(padded_01[batch_size*i:batch_size*(i+1)]) \n",
    "        attention_mask_batch = torch.LongTensor(attention_mask_01[batch_size*i:batch_size*(i+1)])\n",
    "        # Укажем, что градиенты не нужны: модель BERT обучать не будем.\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        # # преобразуем элементы методом numpy() к типу numpy.array\n",
    "        embeddings.append(batch_embeddings[0][:,0,:].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-warren",
   "metadata": {},
   "source": [
    "## Построение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "altered-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Соберём все эмбеддинги в матрицу признаков\n",
    "features = np.concatenate(embeddings)\n",
    "target = df_toxic['toxic']\n",
    "# Разделим данные на тренировочную и тестовую выборки\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.25, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-being",
   "metadata": {},
   "source": [
    "### Модель логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sustainable-generic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера модели логистической регрессии: 0.7165217391304347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/practicum/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/opt/anaconda3/envs/practicum/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/opt/anaconda3/envs/practicum/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/opt/anaconda3/envs/practicum/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/opt/anaconda3/envs/practicum/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression(random_state=12345)\n",
    "result_lr = cross_val_score(model_lr, features_train, target_train,\n",
    "                                 scoring='f1').mean()\n",
    "print('F1-мера модели логистической регрессии:', result_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "overhead-intent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера тестовой выборки 0.7599999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/practicum/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Проверим F1-меру на тестовой выборке\n",
    "model_lr.fit(features_train, target_train)\n",
    "predictions_lr = model_lr.predict(features_test)\n",
    "print('F1-мера тестовой выборки', f1_score(target_test, predictions_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-agent",
   "metadata": {},
   "source": [
    "### Модель LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-signature",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lgbm = LGBMClassifier(random_state=12345)\n",
    "params_lgbm = {'n_estimators' : [100, 200, 500],\n",
    "               'max_depth' : range(2, 7)}\n",
    "\n",
    "search_lgbm = GridSearchCV(estimator=model_lgbm, param_grid=params_lgbm, scoring='f1',\n",
    "                           verbose=0, cv=5)\n",
    "result_lgbm = search_lgbm.fit(features_train , target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "searching-entrance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшее значение F1-меры в LightGBM: 0.6426023252110209\n",
      "Лучшие параметры LightGBM: {'max_depth': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "print('Лучшее значение F1-меры в LightGBM:', result_lgbm.best_score_)\n",
    "print('Лучшие параметры LightGBM:', result_lgbm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "single-agenda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера лучшей модели на тестовой выборке: 0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Проверим модель LightGBM на тестовой выборке\n",
    "best_model_lgbm = search_lgbm.best_estimator_\n",
    "best_model_lgbm.fit(features_train, target_train)\n",
    "predictions_lgbm=best_model_lgbm.predict(features_test)\n",
    "print(\"F1-мера лучшей модели на тестовой выборке:\", f1_score(target_test, predictions_lgbm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-pastor",
   "metadata": {},
   "source": [
    "### Модель CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-extraction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_cat = CatBoostClassifier(random_seed=12345)\n",
    "params_cat = {'iterations' : [100, 300, 500],\n",
    "              'depth' : range(4, 8)}\n",
    "\n",
    "search_cat = GridSearchCV(estimator=model_cat, param_grid=params_cat, scoring='f1',\n",
    "                           verbose=1, cv=3);\n",
    "result_cat = search_cat.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "contained-payday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшее значение F1-меры в CatBoost: 0.4985682597297842\n",
      "Лучшие параметры CatBoost: {'depth': 4, 'iterations': 500}\n"
     ]
    }
   ],
   "source": [
    "print('Лучшее значение F1-меры в CatBoost:', result_cat.best_score_)\n",
    "print('Лучшие параметры CatBoost:', result_cat.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверим модель LightGBM на тестовой выборке\n",
    "best_model_cat = search_cat.best_estimator_\n",
    "best_model_cat.fit(features_train, target_train);\n",
    "predictions_cat=best_model_cat.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "clean-birmingham",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера лучшей модели на тестовой выборке: 0.5714285714285715\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-мера лучшей модели на тестовой выборке:\", f1_score(target_test, predictions_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-enlargement",
   "metadata": {},
   "source": [
    "## Вывод по BERT: \n",
    "Работает долго, поэтому построил модели только на малой части данных (1000 случайных строк). Удивительно, но на логистической регресси каким то образом получилась f1-мера > 0.75=)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-stationery",
   "metadata": {},
   "source": [
    "# Модель без участия BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-variable",
   "metadata": {},
   "source": [
    "## Подготовка данных для классификации текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "unexpected-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим данные\n",
    "df_wikishop = pd.read_csv('df_toxic.csv')\n",
    "corpus = list(df_wikishop['text'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "competent-kentucky",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aleksandrverlan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aleksandrverlan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Напишем функцию для лемматизации текста\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemm(text):   \n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    return' '.join([lemmatizer.lemmatize(w) for w in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "working-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Напишем функцию для очистки текста\n",
    "def clear_text(text):\n",
    "    return ' '.join(re.sub(r'[^a-zA-Z]', ' ', text).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "chemical-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применим функции ко всем строкам\n",
    "df_wikishop['text_lemm'] = df_wikishop['text'].apply(lambda x: lemm(clear_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "assured-reservation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_lemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>D aww He match this background colour I m seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man I m really not trying to edit war It s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>More I can t make any real suggestion on impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  D'aww! He matches this background colour I'm s...      0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                           text_lemm  \n",
       "0  Explanation Why the edits made under my userna...  \n",
       "1  D aww He match this background colour I m seem...  \n",
       "2  Hey man I m really not trying to edit war It s...  \n",
       "3  More I can t make any real suggestion on impro...  \n",
       "4  You sir are my hero Any chance you remember wh...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wikishop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "right-constant",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   text       159571 non-null  object\n",
      " 1   toxic      159571 non-null  int64 \n",
      " 2   text_lemm  159571 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_wikishop.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "regulation-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим данные на тренировочную и тестовую выборки\n",
    "train, test = train_test_split(df_wikishop, test_size=0.25, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "promotional-studio",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aleksandrverlan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "# Выделим переменные признаки и признак, который нужно предсказать\n",
    "train_target=train['toxic']\n",
    "train_corpus=train['text_lemm']\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "train_features = count_tf_idf.fit_transform(train_corpus) \n",
    "test_target=test['toxic']\n",
    "test_corpus=test['text_lemm']\n",
    "test_features = count_tf_idf.transform(test_corpus) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-deficit",
   "metadata": {},
   "source": [
    "## Построение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "egyptian-chambers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим функцию обучения модели\n",
    "def fit_predict(model):\n",
    "    model.fit(train_features, train_target)\n",
    "    predict=model.predict(test_features)\n",
    "    f1=f1_score(test_target, predict)\n",
    "    print(\"F1 модели на тестовой выборке:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-kansas",
   "metadata": {},
   "source": [
    "### Модель логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "appreciated-workplace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия\n",
      "F1 модели на тестовой выборке: 0.7374153664998528\n"
     ]
    }
   ],
   "source": [
    "print('Логистическая регрессия')\n",
    "model_lr = LogisticRegression(random_state=12345)\n",
    "fit_predict(model_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-afternoon",
   "metadata": {},
   "source": [
    "### Модель LigthGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "external-incentive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM\n",
      "F1 модели на тестовой выборке: 0.774986346258875\n"
     ]
    }
   ],
   "source": [
    "print('LightGBM')\n",
    "model_lgb=LGBMClassifier(random_state=12345, n_estimators=500)\n",
    "fit_predict(model_lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-liability",
   "metadata": {},
   "source": [
    "### Модель CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "portuguese-horizon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.150069\n",
      "0:\tlearn: 0.5467665\ttotal: 740ms\tremaining: 6m 9s\n",
      "1:\tlearn: 0.4518392\ttotal: 1.37s\tremaining: 5m 42s\n",
      "2:\tlearn: 0.3855012\ttotal: 1.97s\tremaining: 5m 26s\n",
      "3:\tlearn: 0.3389108\ttotal: 2.47s\tremaining: 5m 6s\n",
      "4:\tlearn: 0.3079332\ttotal: 3.08s\tremaining: 5m 5s\n",
      "5:\tlearn: 0.2842842\ttotal: 3.62s\tremaining: 4m 58s\n",
      "6:\tlearn: 0.2681348\ttotal: 4.18s\tremaining: 4m 54s\n",
      "7:\tlearn: 0.2569842\ttotal: 4.74s\tremaining: 4m 51s\n",
      "8:\tlearn: 0.2478066\ttotal: 5.28s\tremaining: 4m 47s\n",
      "9:\tlearn: 0.2410701\ttotal: 5.86s\tremaining: 4m 47s\n",
      "10:\tlearn: 0.2357826\ttotal: 6.38s\tremaining: 4m 43s\n",
      "11:\tlearn: 0.2302520\ttotal: 6.99s\tremaining: 4m 44s\n",
      "12:\tlearn: 0.2265553\ttotal: 7.52s\tremaining: 4m 41s\n",
      "13:\tlearn: 0.2237080\ttotal: 8.06s\tremaining: 4m 39s\n",
      "14:\tlearn: 0.2209691\ttotal: 8.66s\tremaining: 4m 40s\n",
      "15:\tlearn: 0.2176854\ttotal: 9.16s\tremaining: 4m 37s\n",
      "16:\tlearn: 0.2153899\ttotal: 9.78s\tremaining: 4m 37s\n",
      "17:\tlearn: 0.2135348\ttotal: 10.3s\tremaining: 4m 36s\n",
      "18:\tlearn: 0.2116383\ttotal: 10.9s\tremaining: 4m 35s\n",
      "19:\tlearn: 0.2089918\ttotal: 11.5s\tremaining: 4m 35s\n",
      "20:\tlearn: 0.2072063\ttotal: 12s\tremaining: 4m 33s\n",
      "21:\tlearn: 0.2053590\ttotal: 12.6s\tremaining: 4m 33s\n",
      "22:\tlearn: 0.2040665\ttotal: 13.1s\tremaining: 4m 30s\n",
      "23:\tlearn: 0.2025683\ttotal: 13.7s\tremaining: 4m 31s\n",
      "24:\tlearn: 0.2014835\ttotal: 14.2s\tremaining: 4m 28s\n",
      "25:\tlearn: 0.1998657\ttotal: 14.7s\tremaining: 4m 28s\n",
      "26:\tlearn: 0.1988601\ttotal: 15.2s\tremaining: 4m 26s\n",
      "27:\tlearn: 0.1971499\ttotal: 15.8s\tremaining: 4m 26s\n",
      "28:\tlearn: 0.1961209\ttotal: 16.3s\tremaining: 4m 25s\n",
      "29:\tlearn: 0.1950552\ttotal: 16.9s\tremaining: 4m 23s\n",
      "30:\tlearn: 0.1939756\ttotal: 17.4s\tremaining: 4m 23s\n",
      "31:\tlearn: 0.1930489\ttotal: 17.9s\tremaining: 4m 22s\n",
      "32:\tlearn: 0.1920541\ttotal: 18.5s\tremaining: 4m 21s\n",
      "33:\tlearn: 0.1912035\ttotal: 19s\tremaining: 4m 20s\n",
      "34:\tlearn: 0.1899562\ttotal: 19.6s\tremaining: 4m 20s\n",
      "35:\tlearn: 0.1889831\ttotal: 20.1s\tremaining: 4m 19s\n",
      "36:\tlearn: 0.1882209\ttotal: 20.7s\tremaining: 4m 19s\n",
      "37:\tlearn: 0.1875059\ttotal: 21.2s\tremaining: 4m 17s\n",
      "38:\tlearn: 0.1866030\ttotal: 21.8s\tremaining: 4m 17s\n",
      "39:\tlearn: 0.1855745\ttotal: 22.3s\tremaining: 4m 16s\n",
      "40:\tlearn: 0.1845782\ttotal: 22.9s\tremaining: 4m 16s\n",
      "41:\tlearn: 0.1837269\ttotal: 23.4s\tremaining: 4m 15s\n",
      "42:\tlearn: 0.1828859\ttotal: 24s\tremaining: 4m 14s\n",
      "43:\tlearn: 0.1818012\ttotal: 24.5s\tremaining: 4m 14s\n",
      "44:\tlearn: 0.1811381\ttotal: 25.1s\tremaining: 4m 13s\n",
      "45:\tlearn: 0.1803270\ttotal: 25.6s\tremaining: 4m 12s\n",
      "46:\tlearn: 0.1795553\ttotal: 26.2s\tremaining: 4m 12s\n",
      "47:\tlearn: 0.1788480\ttotal: 26.7s\tremaining: 4m 11s\n",
      "48:\tlearn: 0.1783119\ttotal: 27.3s\tremaining: 4m 10s\n",
      "49:\tlearn: 0.1774814\ttotal: 27.8s\tremaining: 4m 9s\n",
      "50:\tlearn: 0.1770037\ttotal: 28.3s\tremaining: 4m 9s\n",
      "51:\tlearn: 0.1764311\ttotal: 28.8s\tremaining: 4m 8s\n",
      "52:\tlearn: 0.1759495\ttotal: 29.3s\tremaining: 4m 7s\n",
      "53:\tlearn: 0.1752825\ttotal: 29.9s\tremaining: 4m 6s\n",
      "54:\tlearn: 0.1748620\ttotal: 30.4s\tremaining: 4m 6s\n",
      "55:\tlearn: 0.1743711\ttotal: 31s\tremaining: 4m 5s\n",
      "56:\tlearn: 0.1738351\ttotal: 31.5s\tremaining: 4m 4s\n",
      "57:\tlearn: 0.1733248\ttotal: 32s\tremaining: 4m 4s\n",
      "58:\tlearn: 0.1728190\ttotal: 32.6s\tremaining: 4m 3s\n",
      "59:\tlearn: 0.1723506\ttotal: 33.1s\tremaining: 4m 2s\n",
      "60:\tlearn: 0.1719938\ttotal: 33.6s\tremaining: 4m 2s\n",
      "61:\tlearn: 0.1713138\ttotal: 34.2s\tremaining: 4m 1s\n",
      "62:\tlearn: 0.1709319\ttotal: 34.7s\tremaining: 4m\n",
      "63:\tlearn: 0.1704607\ttotal: 35.3s\tremaining: 4m\n",
      "64:\tlearn: 0.1699029\ttotal: 35.9s\tremaining: 3m 59s\n",
      "65:\tlearn: 0.1693911\ttotal: 36.4s\tremaining: 3m 59s\n",
      "66:\tlearn: 0.1687447\ttotal: 36.9s\tremaining: 3m 58s\n",
      "67:\tlearn: 0.1683329\ttotal: 37.5s\tremaining: 3m 58s\n",
      "68:\tlearn: 0.1678883\ttotal: 38s\tremaining: 3m 57s\n",
      "69:\tlearn: 0.1673123\ttotal: 38.5s\tremaining: 3m 56s\n",
      "70:\tlearn: 0.1669331\ttotal: 39.1s\tremaining: 3m 56s\n",
      "71:\tlearn: 0.1663276\ttotal: 39.6s\tremaining: 3m 55s\n",
      "72:\tlearn: 0.1656782\ttotal: 40.2s\tremaining: 3m 54s\n",
      "73:\tlearn: 0.1653102\ttotal: 40.7s\tremaining: 3m 54s\n",
      "74:\tlearn: 0.1649217\ttotal: 41.3s\tremaining: 3m 53s\n",
      "75:\tlearn: 0.1645425\ttotal: 41.8s\tremaining: 3m 53s\n",
      "76:\tlearn: 0.1641586\ttotal: 42.4s\tremaining: 3m 52s\n",
      "77:\tlearn: 0.1634507\ttotal: 42.9s\tremaining: 3m 52s\n",
      "78:\tlearn: 0.1629648\ttotal: 43.5s\tremaining: 3m 51s\n",
      "79:\tlearn: 0.1626435\ttotal: 44s\tremaining: 3m 51s\n",
      "80:\tlearn: 0.1622651\ttotal: 44.6s\tremaining: 3m 50s\n",
      "81:\tlearn: 0.1618922\ttotal: 45.1s\tremaining: 3m 49s\n",
      "82:\tlearn: 0.1615167\ttotal: 45.7s\tremaining: 3m 49s\n",
      "83:\tlearn: 0.1611768\ttotal: 46.2s\tremaining: 3m 48s\n",
      "84:\tlearn: 0.1608172\ttotal: 46.8s\tremaining: 3m 48s\n",
      "85:\tlearn: 0.1604273\ttotal: 47.3s\tremaining: 3m 47s\n",
      "86:\tlearn: 0.1601202\ttotal: 47.9s\tremaining: 3m 47s\n",
      "87:\tlearn: 0.1596969\ttotal: 48.4s\tremaining: 3m 46s\n",
      "88:\tlearn: 0.1593554\ttotal: 48.9s\tremaining: 3m 46s\n",
      "89:\tlearn: 0.1589467\ttotal: 49.5s\tremaining: 3m 45s\n",
      "90:\tlearn: 0.1586301\ttotal: 50.1s\tremaining: 3m 44s\n",
      "91:\tlearn: 0.1581840\ttotal: 50.6s\tremaining: 3m 44s\n",
      "92:\tlearn: 0.1577827\ttotal: 51.2s\tremaining: 3m 43s\n",
      "93:\tlearn: 0.1574655\ttotal: 51.7s\tremaining: 3m 43s\n",
      "94:\tlearn: 0.1571876\ttotal: 52.3s\tremaining: 3m 42s\n",
      "95:\tlearn: 0.1569074\ttotal: 52.8s\tremaining: 3m 42s\n",
      "96:\tlearn: 0.1566315\ttotal: 53.4s\tremaining: 3m 41s\n",
      "97:\tlearn: 0.1562029\ttotal: 53.9s\tremaining: 3m 41s\n",
      "98:\tlearn: 0.1558264\ttotal: 54.5s\tremaining: 3m 40s\n",
      "99:\tlearn: 0.1555953\ttotal: 55s\tremaining: 3m 40s\n",
      "100:\tlearn: 0.1552607\ttotal: 55.5s\tremaining: 3m 39s\n",
      "101:\tlearn: 0.1549398\ttotal: 56.1s\tremaining: 3m 38s\n",
      "102:\tlearn: 0.1546340\ttotal: 56.6s\tremaining: 3m 38s\n",
      "103:\tlearn: 0.1543062\ttotal: 57.2s\tremaining: 3m 37s\n",
      "104:\tlearn: 0.1538768\ttotal: 57.7s\tremaining: 3m 37s\n",
      "105:\tlearn: 0.1535621\ttotal: 58.3s\tremaining: 3m 36s\n",
      "106:\tlearn: 0.1532895\ttotal: 58.8s\tremaining: 3m 36s\n",
      "107:\tlearn: 0.1529899\ttotal: 59.4s\tremaining: 3m 35s\n",
      "108:\tlearn: 0.1524983\ttotal: 59.9s\tremaining: 3m 34s\n",
      "109:\tlearn: 0.1522002\ttotal: 1m\tremaining: 3m 34s\n",
      "110:\tlearn: 0.1519318\ttotal: 1m 1s\tremaining: 3m 33s\n",
      "111:\tlearn: 0.1516726\ttotal: 1m 1s\tremaining: 3m 33s\n",
      "112:\tlearn: 0.1513464\ttotal: 1m 2s\tremaining: 3m 32s\n",
      "113:\tlearn: 0.1510990\ttotal: 1m 2s\tremaining: 3m 32s\n",
      "114:\tlearn: 0.1508574\ttotal: 1m 3s\tremaining: 3m 31s\n",
      "115:\tlearn: 0.1505938\ttotal: 1m 3s\tremaining: 3m 31s\n",
      "116:\tlearn: 0.1503542\ttotal: 1m 4s\tremaining: 3m 30s\n",
      "117:\tlearn: 0.1501585\ttotal: 1m 4s\tremaining: 3m 30s\n",
      "118:\tlearn: 0.1498726\ttotal: 1m 5s\tremaining: 3m 29s\n",
      "119:\tlearn: 0.1496893\ttotal: 1m 5s\tremaining: 3m 28s\n",
      "120:\tlearn: 0.1493725\ttotal: 1m 6s\tremaining: 3m 28s\n",
      "121:\tlearn: 0.1491482\ttotal: 1m 7s\tremaining: 3m 27s\n",
      "122:\tlearn: 0.1488943\ttotal: 1m 7s\tremaining: 3m 27s\n",
      "123:\tlearn: 0.1485738\ttotal: 1m 8s\tremaining: 3m 26s\n",
      "124:\tlearn: 0.1484044\ttotal: 1m 8s\tremaining: 3m 26s\n",
      "125:\tlearn: 0.1481364\ttotal: 1m 9s\tremaining: 3m 25s\n",
      "126:\tlearn: 0.1479121\ttotal: 1m 9s\tremaining: 3m 25s\n",
      "127:\tlearn: 0.1474743\ttotal: 1m 10s\tremaining: 3m 24s\n",
      "128:\tlearn: 0.1472313\ttotal: 1m 11s\tremaining: 3m 24s\n",
      "129:\tlearn: 0.1469898\ttotal: 1m 11s\tremaining: 3m 23s\n",
      "130:\tlearn: 0.1468233\ttotal: 1m 12s\tremaining: 3m 23s\n",
      "131:\tlearn: 0.1466613\ttotal: 1m 12s\tremaining: 3m 22s\n",
      "132:\tlearn: 0.1463677\ttotal: 1m 13s\tremaining: 3m 22s\n",
      "133:\tlearn: 0.1460476\ttotal: 1m 13s\tremaining: 3m 21s\n",
      "134:\tlearn: 0.1458301\ttotal: 1m 14s\tremaining: 3m 20s\n",
      "135:\tlearn: 0.1456388\ttotal: 1m 14s\tremaining: 3m 20s\n",
      "136:\tlearn: 0.1453594\ttotal: 1m 15s\tremaining: 3m 19s\n",
      "137:\tlearn: 0.1451949\ttotal: 1m 15s\tremaining: 3m 19s\n",
      "138:\tlearn: 0.1449689\ttotal: 1m 16s\tremaining: 3m 18s\n",
      "139:\tlearn: 0.1447670\ttotal: 1m 17s\tremaining: 3m 18s\n",
      "140:\tlearn: 0.1444675\ttotal: 1m 17s\tremaining: 3m 17s\n",
      "141:\tlearn: 0.1442172\ttotal: 1m 18s\tremaining: 3m 17s\n",
      "142:\tlearn: 0.1440473\ttotal: 1m 18s\tremaining: 3m 16s\n",
      "143:\tlearn: 0.1437710\ttotal: 1m 19s\tremaining: 3m 15s\n",
      "144:\tlearn: 0.1436132\ttotal: 1m 19s\tremaining: 3m 15s\n",
      "145:\tlearn: 0.1433207\ttotal: 1m 20s\tremaining: 3m 14s\n",
      "146:\tlearn: 0.1430939\ttotal: 1m 20s\tremaining: 3m 14s\n",
      "147:\tlearn: 0.1428437\ttotal: 1m 21s\tremaining: 3m 13s\n",
      "148:\tlearn: 0.1427168\ttotal: 1m 21s\tremaining: 3m 13s\n",
      "149:\tlearn: 0.1425446\ttotal: 1m 22s\tremaining: 3m 12s\n",
      "150:\tlearn: 0.1423239\ttotal: 1m 23s\tremaining: 3m 11s\n",
      "151:\tlearn: 0.1421402\ttotal: 1m 23s\tremaining: 3m 11s\n",
      "152:\tlearn: 0.1420119\ttotal: 1m 24s\tremaining: 3m 10s\n",
      "153:\tlearn: 0.1417624\ttotal: 1m 24s\tremaining: 3m 10s\n",
      "154:\tlearn: 0.1415948\ttotal: 1m 25s\tremaining: 3m 9s\n",
      "155:\tlearn: 0.1413773\ttotal: 1m 25s\tremaining: 3m 9s\n",
      "156:\tlearn: 0.1411626\ttotal: 1m 26s\tremaining: 3m 8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157:\tlearn: 0.1409773\ttotal: 1m 26s\tremaining: 3m 8s\n",
      "158:\tlearn: 0.1408610\ttotal: 1m 27s\tremaining: 3m 7s\n",
      "159:\tlearn: 0.1406999\ttotal: 1m 27s\tremaining: 3m 6s\n",
      "160:\tlearn: 0.1405787\ttotal: 1m 28s\tremaining: 3m 6s\n",
      "161:\tlearn: 0.1403404\ttotal: 1m 29s\tremaining: 3m 5s\n",
      "162:\tlearn: 0.1401237\ttotal: 1m 29s\tremaining: 3m 5s\n",
      "163:\tlearn: 0.1399576\ttotal: 1m 30s\tremaining: 3m 4s\n",
      "164:\tlearn: 0.1398417\ttotal: 1m 30s\tremaining: 3m 4s\n",
      "165:\tlearn: 0.1396628\ttotal: 1m 31s\tremaining: 3m 3s\n",
      "166:\tlearn: 0.1394324\ttotal: 1m 31s\tremaining: 3m 2s\n",
      "167:\tlearn: 0.1392712\ttotal: 1m 32s\tremaining: 3m 2s\n",
      "168:\tlearn: 0.1390109\ttotal: 1m 32s\tremaining: 3m 1s\n",
      "169:\tlearn: 0.1388423\ttotal: 1m 33s\tremaining: 3m 1s\n",
      "170:\tlearn: 0.1384974\ttotal: 1m 33s\tremaining: 3m\n",
      "171:\tlearn: 0.1383837\ttotal: 1m 34s\tremaining: 3m\n",
      "172:\tlearn: 0.1382817\ttotal: 1m 35s\tremaining: 2m 59s\n",
      "173:\tlearn: 0.1381564\ttotal: 1m 35s\tremaining: 2m 59s\n",
      "174:\tlearn: 0.1379561\ttotal: 1m 36s\tremaining: 2m 58s\n",
      "175:\tlearn: 0.1377852\ttotal: 1m 36s\tremaining: 2m 57s\n",
      "176:\tlearn: 0.1376805\ttotal: 1m 37s\tremaining: 2m 57s\n",
      "177:\tlearn: 0.1375779\ttotal: 1m 37s\tremaining: 2m 56s\n",
      "178:\tlearn: 0.1374168\ttotal: 1m 38s\tremaining: 2m 56s\n",
      "179:\tlearn: 0.1371456\ttotal: 1m 38s\tremaining: 2m 55s\n",
      "180:\tlearn: 0.1370480\ttotal: 1m 39s\tremaining: 2m 55s\n",
      "181:\tlearn: 0.1368138\ttotal: 1m 39s\tremaining: 2m 54s\n",
      "182:\tlearn: 0.1366301\ttotal: 1m 40s\tremaining: 2m 54s\n",
      "183:\tlearn: 0.1364897\ttotal: 1m 40s\tremaining: 2m 53s\n",
      "184:\tlearn: 0.1363414\ttotal: 1m 41s\tremaining: 2m 52s\n",
      "185:\tlearn: 0.1362124\ttotal: 1m 42s\tremaining: 2m 52s\n",
      "186:\tlearn: 0.1361128\ttotal: 1m 42s\tremaining: 2m 51s\n",
      "187:\tlearn: 0.1360204\ttotal: 1m 43s\tremaining: 2m 51s\n",
      "188:\tlearn: 0.1357537\ttotal: 1m 43s\tremaining: 2m 50s\n",
      "189:\tlearn: 0.1356521\ttotal: 1m 44s\tremaining: 2m 50s\n",
      "190:\tlearn: 0.1354189\ttotal: 1m 44s\tremaining: 2m 49s\n",
      "191:\tlearn: 0.1352244\ttotal: 1m 45s\tremaining: 2m 49s\n",
      "192:\tlearn: 0.1350020\ttotal: 1m 45s\tremaining: 2m 48s\n",
      "193:\tlearn: 0.1348518\ttotal: 1m 46s\tremaining: 2m 47s\n",
      "194:\tlearn: 0.1347563\ttotal: 1m 46s\tremaining: 2m 47s\n",
      "195:\tlearn: 0.1345932\ttotal: 1m 47s\tremaining: 2m 46s\n",
      "196:\tlearn: 0.1343712\ttotal: 1m 48s\tremaining: 2m 46s\n",
      "197:\tlearn: 0.1341809\ttotal: 1m 48s\tremaining: 2m 45s\n",
      "198:\tlearn: 0.1340449\ttotal: 1m 49s\tremaining: 2m 45s\n",
      "199:\tlearn: 0.1338710\ttotal: 1m 49s\tremaining: 2m 44s\n",
      "200:\tlearn: 0.1337732\ttotal: 1m 50s\tremaining: 2m 43s\n",
      "201:\tlearn: 0.1336962\ttotal: 1m 50s\tremaining: 2m 43s\n",
      "202:\tlearn: 0.1336154\ttotal: 1m 51s\tremaining: 2m 42s\n",
      "203:\tlearn: 0.1334698\ttotal: 1m 51s\tremaining: 2m 42s\n",
      "204:\tlearn: 0.1333430\ttotal: 1m 52s\tremaining: 2m 41s\n",
      "205:\tlearn: 0.1332594\ttotal: 1m 53s\tremaining: 2m 41s\n",
      "206:\tlearn: 0.1331740\ttotal: 1m 53s\tremaining: 2m 40s\n",
      "207:\tlearn: 0.1329981\ttotal: 1m 54s\tremaining: 2m 40s\n",
      "208:\tlearn: 0.1328144\ttotal: 1m 54s\tremaining: 2m 39s\n",
      "209:\tlearn: 0.1326504\ttotal: 1m 55s\tremaining: 2m 39s\n",
      "210:\tlearn: 0.1324911\ttotal: 1m 55s\tremaining: 2m 38s\n",
      "211:\tlearn: 0.1322871\ttotal: 1m 56s\tremaining: 2m 38s\n",
      "212:\tlearn: 0.1322137\ttotal: 1m 56s\tremaining: 2m 37s\n",
      "213:\tlearn: 0.1321270\ttotal: 1m 57s\tremaining: 2m 36s\n",
      "214:\tlearn: 0.1320348\ttotal: 1m 57s\tremaining: 2m 36s\n",
      "215:\tlearn: 0.1317794\ttotal: 1m 58s\tremaining: 2m 35s\n",
      "216:\tlearn: 0.1316346\ttotal: 1m 59s\tremaining: 2m 35s\n",
      "217:\tlearn: 0.1315274\ttotal: 1m 59s\tremaining: 2m 34s\n",
      "218:\tlearn: 0.1314540\ttotal: 2m\tremaining: 2m 34s\n",
      "219:\tlearn: 0.1312808\ttotal: 2m\tremaining: 2m 33s\n",
      "220:\tlearn: 0.1311584\ttotal: 2m 1s\tremaining: 2m 32s\n",
      "221:\tlearn: 0.1309802\ttotal: 2m 1s\tremaining: 2m 32s\n",
      "222:\tlearn: 0.1309012\ttotal: 2m 2s\tremaining: 2m 31s\n",
      "223:\tlearn: 0.1307329\ttotal: 2m 2s\tremaining: 2m 31s\n",
      "224:\tlearn: 0.1306163\ttotal: 2m 3s\tremaining: 2m 30s\n",
      "225:\tlearn: 0.1303147\ttotal: 2m 3s\tremaining: 2m 30s\n",
      "226:\tlearn: 0.1302426\ttotal: 2m 4s\tremaining: 2m 29s\n",
      "227:\tlearn: 0.1301639\ttotal: 2m 4s\tremaining: 2m 29s\n",
      "228:\tlearn: 0.1299734\ttotal: 2m 5s\tremaining: 2m 28s\n",
      "229:\tlearn: 0.1298161\ttotal: 2m 6s\tremaining: 2m 28s\n",
      "230:\tlearn: 0.1297112\ttotal: 2m 6s\tremaining: 2m 27s\n",
      "231:\tlearn: 0.1294542\ttotal: 2m 7s\tremaining: 2m 26s\n",
      "232:\tlearn: 0.1293327\ttotal: 2m 7s\tremaining: 2m 26s\n",
      "233:\tlearn: 0.1291716\ttotal: 2m 8s\tremaining: 2m 25s\n",
      "234:\tlearn: 0.1290438\ttotal: 2m 8s\tremaining: 2m 25s\n",
      "235:\tlearn: 0.1289752\ttotal: 2m 9s\tremaining: 2m 24s\n",
      "236:\tlearn: 0.1289122\ttotal: 2m 9s\tremaining: 2m 24s\n",
      "237:\tlearn: 0.1288368\ttotal: 2m 10s\tremaining: 2m 23s\n",
      "238:\tlearn: 0.1286703\ttotal: 2m 10s\tremaining: 2m 23s\n",
      "239:\tlearn: 0.1286090\ttotal: 2m 11s\tremaining: 2m 22s\n",
      "240:\tlearn: 0.1284393\ttotal: 2m 12s\tremaining: 2m 21s\n",
      "241:\tlearn: 0.1283593\ttotal: 2m 12s\tremaining: 2m 21s\n",
      "242:\tlearn: 0.1282640\ttotal: 2m 13s\tremaining: 2m 20s\n",
      "243:\tlearn: 0.1282052\ttotal: 2m 13s\tremaining: 2m 20s\n",
      "244:\tlearn: 0.1280703\ttotal: 2m 14s\tremaining: 2m 19s\n",
      "245:\tlearn: 0.1279392\ttotal: 2m 14s\tremaining: 2m 19s\n",
      "246:\tlearn: 0.1278752\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "247:\tlearn: 0.1277633\ttotal: 2m 15s\tremaining: 2m 18s\n",
      "248:\tlearn: 0.1276891\ttotal: 2m 16s\tremaining: 2m 17s\n",
      "249:\tlearn: 0.1276268\ttotal: 2m 16s\tremaining: 2m 16s\n",
      "250:\tlearn: 0.1274930\ttotal: 2m 17s\tremaining: 2m 16s\n",
      "251:\tlearn: 0.1273650\ttotal: 2m 18s\tremaining: 2m 15s\n",
      "252:\tlearn: 0.1271728\ttotal: 2m 18s\tremaining: 2m 15s\n",
      "253:\tlearn: 0.1270916\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "254:\tlearn: 0.1270121\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "255:\tlearn: 0.1268832\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "256:\tlearn: 0.1268088\ttotal: 2m 20s\tremaining: 2m 13s\n",
      "257:\tlearn: 0.1266130\ttotal: 2m 21s\tremaining: 2m 12s\n",
      "258:\tlearn: 0.1264541\ttotal: 2m 21s\tremaining: 2m 12s\n",
      "259:\tlearn: 0.1263133\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "260:\tlearn: 0.1262464\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "261:\tlearn: 0.1261034\ttotal: 2m 23s\tremaining: 2m 10s\n",
      "262:\tlearn: 0.1259854\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "263:\tlearn: 0.1258819\ttotal: 2m 24s\tremaining: 2m 9s\n",
      "264:\tlearn: 0.1257523\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "265:\tlearn: 0.1256440\ttotal: 2m 25s\tremaining: 2m 8s\n",
      "266:\tlearn: 0.1255366\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "267:\tlearn: 0.1253318\ttotal: 2m 26s\tremaining: 2m 7s\n",
      "268:\tlearn: 0.1251551\ttotal: 2m 27s\tremaining: 2m 6s\n",
      "269:\tlearn: 0.1250005\ttotal: 2m 27s\tremaining: 2m 6s\n",
      "270:\tlearn: 0.1249432\ttotal: 2m 28s\tremaining: 2m 5s\n",
      "271:\tlearn: 0.1247826\ttotal: 2m 28s\tremaining: 2m 4s\n",
      "272:\tlearn: 0.1247239\ttotal: 2m 29s\tremaining: 2m 4s\n",
      "273:\tlearn: 0.1245885\ttotal: 2m 30s\tremaining: 2m 3s\n",
      "274:\tlearn: 0.1245294\ttotal: 2m 30s\tremaining: 2m 3s\n",
      "275:\tlearn: 0.1242825\ttotal: 2m 31s\tremaining: 2m 2s\n",
      "276:\tlearn: 0.1242239\ttotal: 2m 31s\tremaining: 2m 2s\n",
      "277:\tlearn: 0.1241095\ttotal: 2m 32s\tremaining: 2m 1s\n",
      "278:\tlearn: 0.1239864\ttotal: 2m 32s\tremaining: 2m 1s\n",
      "279:\tlearn: 0.1239013\ttotal: 2m 33s\tremaining: 2m\n",
      "280:\tlearn: 0.1238201\ttotal: 2m 33s\tremaining: 1m 59s\n",
      "281:\tlearn: 0.1237648\ttotal: 2m 34s\tremaining: 1m 59s\n",
      "282:\tlearn: 0.1236052\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "283:\tlearn: 0.1234442\ttotal: 2m 35s\tremaining: 1m 58s\n",
      "284:\tlearn: 0.1232776\ttotal: 2m 36s\tremaining: 1m 57s\n",
      "285:\tlearn: 0.1231708\ttotal: 2m 36s\tremaining: 1m 57s\n",
      "286:\tlearn: 0.1230448\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "287:\tlearn: 0.1229712\ttotal: 2m 37s\tremaining: 1m 56s\n",
      "288:\tlearn: 0.1228629\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "289:\tlearn: 0.1228111\ttotal: 2m 38s\tremaining: 1m 55s\n",
      "290:\tlearn: 0.1227370\ttotal: 2m 39s\tremaining: 1m 54s\n",
      "291:\tlearn: 0.1226411\ttotal: 2m 39s\tremaining: 1m 53s\n",
      "292:\tlearn: 0.1225900\ttotal: 2m 40s\tremaining: 1m 53s\n",
      "293:\tlearn: 0.1224911\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "294:\tlearn: 0.1223711\ttotal: 2m 41s\tremaining: 1m 52s\n",
      "295:\tlearn: 0.1223112\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "296:\tlearn: 0.1222565\ttotal: 2m 42s\tremaining: 1m 51s\n",
      "297:\tlearn: 0.1222052\ttotal: 2m 43s\tremaining: 1m 50s\n",
      "298:\tlearn: 0.1220847\ttotal: 2m 43s\tremaining: 1m 50s\n",
      "299:\tlearn: 0.1220026\ttotal: 2m 44s\tremaining: 1m 49s\n",
      "300:\tlearn: 0.1218763\ttotal: 2m 44s\tremaining: 1m 49s\n",
      "301:\tlearn: 0.1218248\ttotal: 2m 45s\tremaining: 1m 48s\n",
      "302:\tlearn: 0.1217750\ttotal: 2m 46s\tremaining: 1m 47s\n",
      "303:\tlearn: 0.1217288\ttotal: 2m 46s\tremaining: 1m 47s\n",
      "304:\tlearn: 0.1216783\ttotal: 2m 47s\tremaining: 1m 46s\n",
      "305:\tlearn: 0.1216349\ttotal: 2m 47s\tremaining: 1m 46s\n",
      "306:\tlearn: 0.1215194\ttotal: 2m 48s\tremaining: 1m 45s\n",
      "307:\tlearn: 0.1214719\ttotal: 2m 48s\tremaining: 1m 45s\n",
      "308:\tlearn: 0.1214225\ttotal: 2m 49s\tremaining: 1m 44s\n",
      "309:\tlearn: 0.1213723\ttotal: 2m 49s\tremaining: 1m 44s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310:\tlearn: 0.1212715\ttotal: 2m 50s\tremaining: 1m 43s\n",
      "311:\tlearn: 0.1211478\ttotal: 2m 51s\tremaining: 1m 43s\n",
      "312:\tlearn: 0.1210885\ttotal: 2m 51s\tremaining: 1m 42s\n",
      "313:\tlearn: 0.1210435\ttotal: 2m 52s\tremaining: 1m 41s\n",
      "314:\tlearn: 0.1209982\ttotal: 2m 52s\tremaining: 1m 41s\n",
      "315:\tlearn: 0.1209485\ttotal: 2m 53s\tremaining: 1m 40s\n",
      "316:\tlearn: 0.1208720\ttotal: 2m 53s\tremaining: 1m 40s\n",
      "317:\tlearn: 0.1207864\ttotal: 2m 54s\tremaining: 1m 39s\n",
      "318:\tlearn: 0.1206982\ttotal: 2m 54s\tremaining: 1m 39s\n",
      "319:\tlearn: 0.1205861\ttotal: 2m 55s\tremaining: 1m 38s\n",
      "320:\tlearn: 0.1205422\ttotal: 2m 55s\tremaining: 1m 38s\n",
      "321:\tlearn: 0.1204988\ttotal: 2m 56s\tremaining: 1m 37s\n",
      "322:\tlearn: 0.1204556\ttotal: 2m 57s\tremaining: 1m 37s\n",
      "323:\tlearn: 0.1204123\ttotal: 2m 57s\tremaining: 1m 36s\n",
      "324:\tlearn: 0.1203160\ttotal: 2m 58s\tremaining: 1m 35s\n",
      "325:\tlearn: 0.1202127\ttotal: 2m 58s\tremaining: 1m 35s\n",
      "326:\tlearn: 0.1201554\ttotal: 2m 59s\tremaining: 1m 34s\n",
      "327:\tlearn: 0.1201111\ttotal: 3m\tremaining: 1m 34s\n",
      "328:\tlearn: 0.1199751\ttotal: 3m\tremaining: 1m 34s\n",
      "329:\tlearn: 0.1199234\ttotal: 3m 1s\tremaining: 1m 33s\n",
      "330:\tlearn: 0.1198677\ttotal: 3m 2s\tremaining: 1m 33s\n",
      "331:\tlearn: 0.1197332\ttotal: 3m 3s\tremaining: 1m 32s\n",
      "332:\tlearn: 0.1196490\ttotal: 3m 4s\tremaining: 1m 32s\n",
      "333:\tlearn: 0.1195479\ttotal: 3m 4s\tremaining: 1m 31s\n",
      "334:\tlearn: 0.1194384\ttotal: 3m 5s\tremaining: 1m 31s\n",
      "335:\tlearn: 0.1193130\ttotal: 3m 5s\tremaining: 1m 30s\n",
      "336:\tlearn: 0.1192147\ttotal: 3m 6s\tremaining: 1m 30s\n",
      "337:\tlearn: 0.1191724\ttotal: 3m 6s\tremaining: 1m 29s\n",
      "338:\tlearn: 0.1191245\ttotal: 3m 7s\tremaining: 1m 28s\n",
      "339:\tlearn: 0.1190790\ttotal: 3m 8s\tremaining: 1m 28s\n",
      "340:\tlearn: 0.1189417\ttotal: 3m 8s\tremaining: 1m 28s\n",
      "341:\tlearn: 0.1187931\ttotal: 3m 9s\tremaining: 1m 27s\n",
      "342:\tlearn: 0.1187129\ttotal: 3m 9s\tremaining: 1m 26s\n",
      "343:\tlearn: 0.1186596\ttotal: 3m 10s\tremaining: 1m 26s\n",
      "344:\tlearn: 0.1184868\ttotal: 3m 10s\tremaining: 1m 25s\n",
      "345:\tlearn: 0.1184446\ttotal: 3m 11s\tremaining: 1m 25s\n",
      "346:\tlearn: 0.1183628\ttotal: 3m 11s\tremaining: 1m 24s\n",
      "347:\tlearn: 0.1182318\ttotal: 3m 12s\tremaining: 1m 24s\n",
      "348:\tlearn: 0.1181891\ttotal: 3m 12s\tremaining: 1m 23s\n",
      "349:\tlearn: 0.1181473\ttotal: 3m 13s\tremaining: 1m 22s\n",
      "350:\tlearn: 0.1181058\ttotal: 3m 13s\tremaining: 1m 22s\n",
      "351:\tlearn: 0.1180659\ttotal: 3m 14s\tremaining: 1m 21s\n",
      "352:\tlearn: 0.1180268\ttotal: 3m 14s\tremaining: 1m 21s\n",
      "353:\tlearn: 0.1179286\ttotal: 3m 15s\tremaining: 1m 20s\n",
      "354:\tlearn: 0.1178678\ttotal: 3m 15s\tremaining: 1m 19s\n",
      "355:\tlearn: 0.1178036\ttotal: 3m 16s\tremaining: 1m 19s\n",
      "356:\tlearn: 0.1177612\ttotal: 3m 16s\tremaining: 1m 18s\n",
      "357:\tlearn: 0.1177216\ttotal: 3m 16s\tremaining: 1m 18s\n",
      "358:\tlearn: 0.1176837\ttotal: 3m 17s\tremaining: 1m 17s\n",
      "359:\tlearn: 0.1175772\ttotal: 3m 17s\tremaining: 1m 16s\n",
      "360:\tlearn: 0.1175376\ttotal: 3m 18s\tremaining: 1m 16s\n",
      "361:\tlearn: 0.1175006\ttotal: 3m 18s\tremaining: 1m 15s\n",
      "362:\tlearn: 0.1174176\ttotal: 3m 19s\tremaining: 1m 15s\n",
      "363:\tlearn: 0.1172155\ttotal: 3m 19s\tremaining: 1m 14s\n",
      "364:\tlearn: 0.1171610\ttotal: 3m 19s\tremaining: 1m 13s\n",
      "365:\tlearn: 0.1171091\ttotal: 3m 20s\tremaining: 1m 13s\n",
      "366:\tlearn: 0.1170634\ttotal: 3m 20s\tremaining: 1m 12s\n",
      "367:\tlearn: 0.1170039\ttotal: 3m 21s\tremaining: 1m 12s\n",
      "368:\tlearn: 0.1169620\ttotal: 3m 21s\tremaining: 1m 11s\n",
      "369:\tlearn: 0.1168585\ttotal: 3m 22s\tremaining: 1m 11s\n",
      "370:\tlearn: 0.1167773\ttotal: 3m 22s\tremaining: 1m 10s\n",
      "371:\tlearn: 0.1167380\ttotal: 3m 22s\tremaining: 1m 9s\n",
      "372:\tlearn: 0.1167000\ttotal: 3m 23s\tremaining: 1m 9s\n",
      "373:\tlearn: 0.1165785\ttotal: 3m 23s\tremaining: 1m 8s\n",
      "374:\tlearn: 0.1165208\ttotal: 3m 24s\tremaining: 1m 8s\n",
      "375:\tlearn: 0.1163944\ttotal: 3m 24s\tremaining: 1m 7s\n",
      "376:\tlearn: 0.1163504\ttotal: 3m 25s\tremaining: 1m 6s\n",
      "377:\tlearn: 0.1163120\ttotal: 3m 25s\tremaining: 1m 6s\n",
      "378:\tlearn: 0.1162727\ttotal: 3m 25s\tremaining: 1m 5s\n",
      "379:\tlearn: 0.1161690\ttotal: 3m 26s\tremaining: 1m 5s\n",
      "380:\tlearn: 0.1160546\ttotal: 3m 26s\tremaining: 1m 4s\n",
      "381:\tlearn: 0.1159353\ttotal: 3m 27s\tremaining: 1m 4s\n",
      "382:\tlearn: 0.1158960\ttotal: 3m 27s\tremaining: 1m 3s\n",
      "383:\tlearn: 0.1157722\ttotal: 3m 28s\tremaining: 1m 2s\n",
      "384:\tlearn: 0.1157364\ttotal: 3m 28s\tremaining: 1m 2s\n",
      "385:\tlearn: 0.1156570\ttotal: 3m 29s\tremaining: 1m 1s\n",
      "386:\tlearn: 0.1155526\ttotal: 3m 29s\tremaining: 1m 1s\n",
      "387:\tlearn: 0.1154897\ttotal: 3m 30s\tremaining: 1m\n",
      "388:\tlearn: 0.1154500\ttotal: 3m 30s\tremaining: 1m\n",
      "389:\tlearn: 0.1153983\ttotal: 3m 31s\tremaining: 59.7s\n",
      "390:\tlearn: 0.1152891\ttotal: 3m 32s\tremaining: 59.1s\n",
      "391:\tlearn: 0.1152446\ttotal: 3m 32s\tremaining: 58.6s\n",
      "392:\tlearn: 0.1150251\ttotal: 3m 33s\tremaining: 58s\n",
      "393:\tlearn: 0.1149477\ttotal: 3m 33s\tremaining: 57.5s\n",
      "394:\tlearn: 0.1148875\ttotal: 3m 34s\tremaining: 56.9s\n",
      "395:\tlearn: 0.1148535\ttotal: 3m 34s\tremaining: 56.3s\n",
      "396:\tlearn: 0.1148176\ttotal: 3m 34s\tremaining: 55.7s\n",
      "397:\tlearn: 0.1147821\ttotal: 3m 35s\tremaining: 55.2s\n",
      "398:\tlearn: 0.1146931\ttotal: 3m 35s\tremaining: 54.6s\n",
      "399:\tlearn: 0.1146545\ttotal: 3m 36s\tremaining: 54s\n",
      "400:\tlearn: 0.1145500\ttotal: 3m 36s\tremaining: 53.5s\n",
      "401:\tlearn: 0.1145148\ttotal: 3m 37s\tremaining: 52.9s\n",
      "402:\tlearn: 0.1144781\ttotal: 3m 37s\tremaining: 52.3s\n",
      "403:\tlearn: 0.1144236\ttotal: 3m 37s\tremaining: 51.8s\n",
      "404:\tlearn: 0.1143722\ttotal: 3m 38s\tremaining: 51.3s\n",
      "405:\tlearn: 0.1143343\ttotal: 3m 39s\tremaining: 50.8s\n",
      "406:\tlearn: 0.1142951\ttotal: 3m 40s\tremaining: 50.4s\n",
      "407:\tlearn: 0.1142362\ttotal: 3m 41s\tremaining: 49.9s\n",
      "408:\tlearn: 0.1141122\ttotal: 3m 41s\tremaining: 49.4s\n",
      "409:\tlearn: 0.1140785\ttotal: 3m 42s\tremaining: 48.9s\n",
      "410:\tlearn: 0.1140307\ttotal: 3m 43s\tremaining: 48.4s\n",
      "411:\tlearn: 0.1139211\ttotal: 3m 44s\tremaining: 47.9s\n",
      "412:\tlearn: 0.1138468\ttotal: 3m 44s\tremaining: 47.4s\n",
      "413:\tlearn: 0.1137278\ttotal: 3m 45s\tremaining: 46.8s\n",
      "414:\tlearn: 0.1136131\ttotal: 3m 46s\tremaining: 46.3s\n",
      "415:\tlearn: 0.1134618\ttotal: 3m 46s\tremaining: 45.8s\n",
      "416:\tlearn: 0.1134128\ttotal: 3m 47s\tremaining: 45.3s\n",
      "417:\tlearn: 0.1133782\ttotal: 3m 48s\tremaining: 44.8s\n",
      "418:\tlearn: 0.1132734\ttotal: 3m 48s\tremaining: 44.2s\n",
      "419:\tlearn: 0.1132397\ttotal: 3m 49s\tremaining: 43.7s\n",
      "420:\tlearn: 0.1131610\ttotal: 3m 49s\tremaining: 43.1s\n",
      "421:\tlearn: 0.1130248\ttotal: 3m 50s\tremaining: 42.6s\n",
      "422:\tlearn: 0.1129908\ttotal: 3m 51s\tremaining: 42.1s\n",
      "423:\tlearn: 0.1129569\ttotal: 3m 51s\tremaining: 41.5s\n",
      "424:\tlearn: 0.1128341\ttotal: 3m 52s\tremaining: 41s\n",
      "425:\tlearn: 0.1127531\ttotal: 3m 52s\tremaining: 40.5s\n",
      "426:\tlearn: 0.1127135\ttotal: 3m 53s\tremaining: 39.9s\n",
      "427:\tlearn: 0.1126333\ttotal: 3m 54s\tremaining: 39.4s\n",
      "428:\tlearn: 0.1125986\ttotal: 3m 54s\tremaining: 38.8s\n",
      "429:\tlearn: 0.1125574\ttotal: 3m 55s\tremaining: 38.3s\n",
      "430:\tlearn: 0.1124495\ttotal: 3m 55s\tremaining: 37.8s\n",
      "431:\tlearn: 0.1123426\ttotal: 3m 56s\tremaining: 37.3s\n",
      "432:\tlearn: 0.1123083\ttotal: 3m 57s\tremaining: 36.7s\n",
      "433:\tlearn: 0.1122502\ttotal: 3m 57s\tremaining: 36.2s\n",
      "434:\tlearn: 0.1121860\ttotal: 3m 58s\tremaining: 35.7s\n",
      "435:\tlearn: 0.1120822\ttotal: 3m 59s\tremaining: 35.1s\n",
      "436:\tlearn: 0.1120237\ttotal: 3m 59s\tremaining: 34.6s\n",
      "437:\tlearn: 0.1119909\ttotal: 4m\tremaining: 34s\n",
      "438:\tlearn: 0.1119482\ttotal: 4m\tremaining: 33.5s\n",
      "439:\tlearn: 0.1119187\ttotal: 4m 1s\tremaining: 32.9s\n",
      "440:\tlearn: 0.1118862\ttotal: 4m 1s\tremaining: 32.4s\n",
      "441:\tlearn: 0.1118524\ttotal: 4m 2s\tremaining: 31.8s\n",
      "442:\tlearn: 0.1118208\ttotal: 4m 3s\tremaining: 31.3s\n",
      "443:\tlearn: 0.1117046\ttotal: 4m 3s\tremaining: 30.7s\n",
      "444:\tlearn: 0.1116354\ttotal: 4m 4s\tremaining: 30.2s\n",
      "445:\tlearn: 0.1115357\ttotal: 4m 4s\tremaining: 29.6s\n",
      "446:\tlearn: 0.1115056\ttotal: 4m 5s\tremaining: 29.1s\n",
      "447:\tlearn: 0.1113983\ttotal: 4m 6s\tremaining: 28.6s\n",
      "448:\tlearn: 0.1113666\ttotal: 4m 6s\tremaining: 28s\n",
      "449:\tlearn: 0.1113097\ttotal: 4m 7s\tremaining: 27.5s\n",
      "450:\tlearn: 0.1112188\ttotal: 4m 8s\tremaining: 27s\n",
      "451:\tlearn: 0.1111736\ttotal: 4m 8s\tremaining: 26.4s\n",
      "452:\tlearn: 0.1110645\ttotal: 4m 9s\tremaining: 25.9s\n",
      "453:\tlearn: 0.1109840\ttotal: 4m 10s\tremaining: 25.4s\n",
      "454:\tlearn: 0.1109529\ttotal: 4m 11s\tremaining: 24.8s\n",
      "455:\tlearn: 0.1109107\ttotal: 4m 11s\tremaining: 24.3s\n",
      "456:\tlearn: 0.1108674\ttotal: 4m 12s\tremaining: 23.7s\n",
      "457:\tlearn: 0.1108381\ttotal: 4m 12s\tremaining: 23.2s\n",
      "458:\tlearn: 0.1108007\ttotal: 4m 13s\tremaining: 22.6s\n",
      "459:\tlearn: 0.1107697\ttotal: 4m 13s\tremaining: 22.1s\n",
      "460:\tlearn: 0.1107092\ttotal: 4m 14s\tremaining: 21.5s\n",
      "461:\tlearn: 0.1106442\ttotal: 4m 14s\tremaining: 21s\n",
      "462:\tlearn: 0.1105445\ttotal: 4m 15s\tremaining: 20.4s\n",
      "463:\tlearn: 0.1103999\ttotal: 4m 15s\tremaining: 19.9s\n",
      "464:\tlearn: 0.1103683\ttotal: 4m 16s\tremaining: 19.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465:\tlearn: 0.1103269\ttotal: 4m 16s\tremaining: 18.7s\n",
      "466:\tlearn: 0.1102896\ttotal: 4m 17s\tremaining: 18.2s\n",
      "467:\tlearn: 0.1101379\ttotal: 4m 18s\tremaining: 17.6s\n",
      "468:\tlearn: 0.1100638\ttotal: 4m 18s\tremaining: 17.1s\n",
      "469:\tlearn: 0.1100344\ttotal: 4m 19s\tremaining: 16.5s\n",
      "470:\tlearn: 0.1099230\ttotal: 4m 19s\tremaining: 16s\n",
      "471:\tlearn: 0.1098259\ttotal: 4m 20s\tremaining: 15.4s\n",
      "472:\tlearn: 0.1097822\ttotal: 4m 20s\tremaining: 14.9s\n",
      "473:\tlearn: 0.1096510\ttotal: 4m 21s\tremaining: 14.3s\n",
      "474:\tlearn: 0.1096233\ttotal: 4m 22s\tremaining: 13.8s\n",
      "475:\tlearn: 0.1095307\ttotal: 4m 22s\tremaining: 13.2s\n",
      "476:\tlearn: 0.1094643\ttotal: 4m 23s\tremaining: 12.7s\n",
      "477:\tlearn: 0.1093613\ttotal: 4m 23s\tremaining: 12.1s\n",
      "478:\tlearn: 0.1093300\ttotal: 4m 24s\tremaining: 11.6s\n",
      "479:\tlearn: 0.1092788\ttotal: 4m 24s\tremaining: 11s\n",
      "480:\tlearn: 0.1092082\ttotal: 4m 25s\tremaining: 10.5s\n",
      "481:\tlearn: 0.1091784\ttotal: 4m 25s\tremaining: 9.93s\n",
      "482:\tlearn: 0.1090883\ttotal: 4m 26s\tremaining: 9.38s\n",
      "483:\tlearn: 0.1090196\ttotal: 4m 27s\tremaining: 8.83s\n",
      "484:\tlearn: 0.1089822\ttotal: 4m 27s\tremaining: 8.28s\n",
      "485:\tlearn: 0.1089271\ttotal: 4m 28s\tremaining: 7.72s\n",
      "486:\tlearn: 0.1088960\ttotal: 4m 28s\tremaining: 7.17s\n",
      "487:\tlearn: 0.1087974\ttotal: 4m 29s\tremaining: 6.62s\n",
      "488:\tlearn: 0.1087466\ttotal: 4m 29s\tremaining: 6.07s\n",
      "489:\tlearn: 0.1087184\ttotal: 4m 30s\tremaining: 5.52s\n",
      "490:\tlearn: 0.1086795\ttotal: 4m 30s\tremaining: 4.97s\n",
      "491:\tlearn: 0.1086493\ttotal: 4m 31s\tremaining: 4.41s\n",
      "492:\tlearn: 0.1086094\ttotal: 4m 32s\tremaining: 3.86s\n",
      "493:\tlearn: 0.1085806\ttotal: 4m 32s\tremaining: 3.31s\n",
      "494:\tlearn: 0.1085540\ttotal: 4m 33s\tremaining: 2.76s\n",
      "495:\tlearn: 0.1084453\ttotal: 4m 33s\tremaining: 2.21s\n",
      "496:\tlearn: 0.1083443\ttotal: 4m 34s\tremaining: 1.66s\n",
      "497:\tlearn: 0.1083062\ttotal: 4m 34s\tremaining: 1.1s\n",
      "498:\tlearn: 0.1081958\ttotal: 4m 35s\tremaining: 552ms\n",
      "499:\tlearn: 0.1081685\ttotal: 4m 35s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "print('CatBoost')\n",
    "model_cat=CatBoostClassifier(random_seed=12345, iterations=500)\n",
    "model_cat.fit(train_features, train_target)\n",
    "predict=model_cat.predict(test_features)\n",
    "f1=f1_score(test_target, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "italic-complement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 модели на тестовой выборке: 0.7515341801056087\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 модели на тестовой выборке:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-bridge",
   "metadata": {},
   "source": [
    "Модели LightGBM и CatBoost выдали F1-меру > 0.75, но на наших данных LightGBM выиграла=) Логистическая регрессия была близка к ним!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-population",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "- В ходе работы мы изучили данные\n",
    "- Подготовили малую часть данных (1000 строк) с помощью предобученной модели DistilBERT\n",
    "- Обучили различные модели: логистическую регрессию, LightGBM, CatBoost, и сравнили их F1-меру\n",
    "- Подготовили данные с помощью TF-IDF и обучили те же модели на новых данных\n",
    "- Получили F1-меру = 0.77 в модели LightGBM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "208px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
